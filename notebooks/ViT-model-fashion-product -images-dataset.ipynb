{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Fine-tune the Vision Transformer on Fashion Product Images\n\n\nIn this notebook, we are going to fine-tune a pre-trained [Vision Transformer](https://huggingface.co/docs/transformers/model_doc/vit) (which I added to [Transformers](https://github.com/huggingface/transformers)) on the Fashion Product Images dataset.\n\nWe will prepare the data using [datasets](https://github.com/huggingface/datasets), and train the model using the [Trainer](https://huggingface.co/transformers/main_classes/trainer.html). For other notebooks (such as training ViT with PyTorch Lightning), I refer to my repo [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials). \n\n","metadata":{}},{"cell_type":"markdown","source":"Let's start by installing the relevant libraries.\n","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:03:58.327295Z","iopub.execute_input":"2023-05-01T03:03:58.327581Z","iopub.status.idle":"2023-05-01T03:04:11.468418Z","shell.execute_reply.started":"2023-05-01T03:03:58.327548Z","shell.execute_reply":"2023-05-01T03:04:11.467186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data\nHere we load Fashion Products, a famous fashion image dataset, from HuggingFace's [hub](https://huggingface.co/datasets/ceyda/fashion-products-small). Note that you can view all examples of the dataset directly in your browser!\n\nNote: if you want to load your custom image classification dataset, check out [this guide](https://huggingface.co/docs/datasets/image_load).\n\nFor demonstration purposes, we will only actually use a small portion.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_ds = load_dataset('ceyda/fashion-products-small')","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:05:52.392860Z","iopub.execute_input":"2023-05-01T03:05:52.393494Z","iopub.status.idle":"2023-05-01T03:06:17.962787Z","shell.execute_reply.started":"2023-05-01T03:05:52.393450Z","shell.execute_reply":"2023-05-01T03:06:17.961636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = train_ds['train'].train_test_split(test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:06:55.790430Z","iopub.execute_input":"2023-05-01T03:06:55.791332Z","iopub.status.idle":"2023-05-01T03:06:55.832823Z","shell.execute_reply.started":"2023-05-01T03:06:55.791289Z","shell.execute_reply":"2023-05-01T03:06:55.831710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:07:32.502403Z","iopub.execute_input":"2023-05-01T03:07:32.502783Z","iopub.status.idle":"2023-05-01T03:07:32.511210Z","shell.execute_reply.started":"2023-05-01T03:07:32.502747Z","shell.execute_reply":"2023-05-01T03:07:32.509993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_ds['train']\ntest_data = train_ds['test']","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:07:23.507705Z","iopub.execute_input":"2023-05-01T03:07:23.508078Z","iopub.status.idle":"2023-05-01T03:07:23.513783Z","shell.execute_reply.started":"2023-05-01T03:07:23.508046Z","shell.execute_reply":"2023-05-01T03:07:23.512616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[200]['image']","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:07:59.710293Z","iopub.execute_input":"2023-05-01T03:07:59.710962Z","iopub.status.idle":"2023-05-01T03:07:59.747479Z","shell.execute_reply.started":"2023-05-01T03:07:59.710921Z","shell.execute_reply":"2023-05-01T03:07:59.746362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course, we would like to know the actual class name, rather than the \n\n---\n\ninteger index. We can obtain that by creating a dictionary which maps between integer indices and actual class names (id2label):","metadata":{}},{"cell_type":"code","source":"label = list(set(train_data['masterCategory']))\nid2label = {id:label for id, label in enumerate(label)}\nlabel2id = {label:id for id,label in id2label.items()}\nprint(id2label, label2id)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:08:52.018373Z","iopub.execute_input":"2023-05-01T03:08:52.018774Z","iopub.status.idle":"2023-05-01T03:08:52.194057Z","shell.execute_reply.started":"2023-05-01T03:08:52.018735Z","shell.execute_reply":"2023-05-01T03:08:52.192724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing the data\n\nWe will now preprocess the data. The model requires 2 things: `pixel_values` and `labels`. \n\nWe will perform data augmentaton **on-the-fly** using HuggingFace Datasets' `set_transform` method (docs can be found [here](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=set_transform#datasets.Dataset.set_transform)). This method is kind of a lazy `map`: the transform is only applied when examples are accessed. This is convenient for tokenizing or padding text, or augmenting images at training time for example, as we will do here. ","metadata":{}},{"cell_type":"code","source":"from transformers import ViTImageProcessor\n\nprocessor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:09:28.736141Z","iopub.execute_input":"2023-05-01T03:09:28.737111Z","iopub.status.idle":"2023-05-01T03:09:38.078364Z","shell.execute_reply.started":"2023-05-01T03:09:28.737056Z","shell.execute_reply":"2023-05-01T03:09:38.077184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For data augmentation, one can use any available library. Here we'll use torchvision's [transforms module](https://pytorch.org/vision/stable/transforms.html).","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import (CenterCrop, \n                                    Compose, \n                                    Normalize, \n                                    RandomHorizontalFlip,\n                                    RandomResizedCrop, \n                                    Resize, \n                                    ToTensor)\n\nimage_mean, image_std = processor.image_mean, processor.image_std\nsize = processor.size[\"height\"]\nprint(\"Size: \", size)\n\nnormalize = Normalize(mean=image_mean, std=image_std)\n_train_transforms = Compose(\n        [\n            Resize((size, size)),\n            RandomHorizontalFlip(),\n            ToTensor(),\n            normalize,\n        ]\n    )\n\n_val_transforms = Compose(\n        [\n            Resize((size, size)),\n            # CenterCrop(size),\n            ToTensor(),\n            normalize,\n        ]\n    )\n\ndef train_transforms(examples):\n    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n    return examples\n\ndef val_transforms(examples):\n    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n    return examples","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:09:56.034602Z","iopub.execute_input":"2023-05-01T03:09:56.035573Z","iopub.status.idle":"2023-05-01T03:09:56.630778Z","shell.execute_reply.started":"2023-05-01T03:09:56.035517Z","shell.execute_reply":"2023-05-01T03:09:56.629440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the transforms\ntrain_data.set_transform(train_transforms)\n# val_ds.set_transform(val_transforms)\ntest_data.set_transform(val_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:10:14.066581Z","iopub.execute_input":"2023-05-01T03:10:14.067308Z","iopub.status.idle":"2023-05-01T03:10:14.074321Z","shell.execute_reply.started":"2023-05-01T03:10:14.067271Z","shell.execute_reply":"2023-05-01T03:10:14.072854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's very easy to create a corresponding PyTorch DataLoader, like so:","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\n\ndef collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    labels = torch.tensor([label2id[example[\"masterCategory\"]] for example in examples])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ntrain_dataloader = DataLoader(train_data, collate_fn=collate_fn, batch_size=4)\ntest_dataloader = DataLoader(test_data, collate_fn=collate_fn, batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:11:20.691005Z","iopub.execute_input":"2023-05-01T03:11:20.692184Z","iopub.status.idle":"2023-05-01T03:11:20.699349Z","shell.execute_reply.started":"2023-05-01T03:11:20.692132Z","shell.execute_reply":"2023-05-01T03:11:20.698164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n  if isinstance(v, torch.Tensor):\n    print(k, v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:11:26.539089Z","iopub.execute_input":"2023-05-01T03:11:26.539529Z","iopub.status.idle":"2023-05-01T03:11:26.581761Z","shell.execute_reply.started":"2023-05-01T03:11:26.539490Z","shell.execute_reply":"2023-05-01T03:11:26.580662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(test_dataloader))\nfor k,v in batch.items():\n  if isinstance(v, torch.Tensor):\n    print(k, v.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:11:31.762878Z","iopub.execute_input":"2023-05-01T03:11:31.763482Z","iopub.status.idle":"2023-05-01T03:11:31.801584Z","shell.execute_reply.started":"2023-05-01T03:11:31.763435Z","shell.execute_reply":"2023-05-01T03:11:31.800435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, we'll not be using dataloaders, as we are going to use HuggingFace's Trainer API.","metadata":{}},{"cell_type":"markdown","source":"## Define the model\n\nHere we define the model. We define a `ViTForImageClassification`, which places a linear layer ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) on top of a pre-trained `ViTModel`. The linear layer is placed on top of the last hidden state of the [CLS] token, which serves as a good representation of an entire image. \n\nThe model itself is pre-trained on ImageNet-21k, a dataset of 14 million labeled images. You can find all info of the model we are going to use [here](https://huggingface.co/google/vit-base-patch16-224-in21k).\n\nWe also specify the number of output neurons by setting the id2label and label2id mapping, which we be added as attributes to the configuration of the model (which can be accessed as `model.config`).","metadata":{}},{"cell_type":"code","source":"from transformers import ViTForImageClassification\n\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n                                                  id2label=id2label,\n                                                  label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:12:09.785580Z","iopub.execute_input":"2023-05-01T03:12:09.786346Z","iopub.status.idle":"2023-05-01T03:12:13.105393Z","shell.execute_reply.started":"2023-05-01T03:12:09.786303Z","shell.execute_reply":"2023-05-01T03:12:13.104404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To instantiate a `Trainer`, we will need to define three more things. The most important is the `TrainingArguments`, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional.\n\nWe also set the argument \"remove_unused_columns\" to False, because otherwise the \"img\" column would be removed, which is required for the data transformations.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nmetric_name = \"accuracy\"\n\nargs = TrainingArguments(\n    \"Fashion-Product-Images\",\n    save_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5, #0.00002\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=metric_name,\n    logging_dir='logs',\n    remove_unused_columns=False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:12:33.048600Z","iopub.execute_input":"2023-05-01T03:12:33.049286Z","iopub.status.idle":"2023-05-01T03:12:33.441110Z","shell.execute_reply.started":"2023-05-01T03:12:33.049244Z","shell.execute_reply":"2023-05-01T03:12:33.440101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, set the training and evaluation batch_sizes and customize the number of epochs for training, as well as the weight decay.\n\nWe also define a `compute_metrics` function that will be used to compute metrics at evaluation. We use \"accuracy\" here.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return dict(accuracy=accuracy_score(predictions, labels))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:12:55.731035Z","iopub.execute_input":"2023-05-01T03:12:55.731417Z","iopub.status.idle":"2023-05-01T03:12:55.737478Z","shell.execute_reply.started":"2023-05-01T03:12:55.731375Z","shell.execute_reply":"2023-05-01T03:12:55.736114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    tokenizer=processor,\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:13:14.894818Z","iopub.execute_input":"2023-05-01T03:13:14.895606Z","iopub.status.idle":"2023-05-01T03:13:19.228785Z","shell.execute_reply.started":"2023-05-01T03:13:14.895554Z","shell.execute_reply":"2023-05-01T03:13:19.227706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model\n\nLet's first start up Tensorboard:","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T03:13:33.901209Z","iopub.execute_input":"2023-05-01T03:13:33.901584Z","iopub.status.idle":"2023-05-01T04:02:50.509449Z","shell.execute_reply.started":"2023-05-01T03:13:33.901549Z","shell.execute_reply":"2023-05-01T04:02:50.508479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n\nFinally, let's evaluate the model on the test set:","metadata":{}},{"cell_type":"code","source":"outputs = trainer.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T04:06:58.787759Z","iopub.execute_input":"2023-05-01T04:06:58.788507Z","iopub.status.idle":"2023-05-01T04:08:24.143063Z","shell.execute_reply.started":"2023-05-01T04:06:58.788464Z","shell.execute_reply":"2023-05-01T04:08:24.139582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(outputs.metrics)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T04:08:30.024923Z","iopub.execute_input":"2023-05-01T04:08:30.025512Z","iopub.status.idle":"2023-05-01T04:08:30.034610Z","shell.execute_reply.started":"2023-05-01T04:08:30.025472Z","shell.execute_reply":"2023-05-01T04:08:30.033230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}